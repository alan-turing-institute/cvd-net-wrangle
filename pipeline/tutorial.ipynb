{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756043f1-ec1f-4b83-9c58-50add268ac4d",
   "metadata": {},
   "source": [
    "# CVD-Net Database Pipeline: Tutorial \n",
    "\n",
    "This is a work in progress tutorial notebook. It guides a user in how to create a database schema in postgreSQL (**psql**), and then uses **Python** functions to load in a data dictionary from a CSV file into a **psql** table. Below are some brief notes on how to this works - more context will be added soon. \n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. We are writing and executing this code on our local machines (**MacOS**), with **psql** installed via [Homebrew](https://www.postgresql.org/download/macosx/). It is important to note that the database set-up and interaction could be different once on the TRE. \n",
    "2. We are working within a virtual env (`conda create --name cvdnet python=3.13`)\n",
    "3. We installed these packages in the cvdnet virtual env (`conda install sqlalchemy pandas ipykernel psycopg2 openpyxl`)\n",
    "4. We are using the **Visal Studio Code** IDE with [PostgreSQL Explorer extension](https://marketplace.visualstudio.com/items?itemName=ckolkman.vscode-postgres) - not essential, but allows for nice visualisation of the database. \n",
    "5. We created the schema on the shell terminal, and checked we could view the tables:\n",
    "   - `dropdb CVD-Net` \n",
    "   - `createdb CVD-Net` \n",
    "   - `psql -f pipeline/CVD-net_consolidated_DDL.sql CVD-Net`\n",
    "6. Set up the database connection in the PostgreSQL Explorer extension, in order to visualise the database and query the tables or the views (right click). \n",
    "   - Click the plus symbol to 'Add Connection'\n",
    "   - hostname: localhost\n",
    "   - user: your postgres username\n",
    "   - password: left blank for now\n",
    "   - port: use default\n",
    "   - ssl connection: use default\n",
    "   - database to connect to: CVD-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running main.py will import all other parts of the pipeline\n",
    "# The database engine is generated upon loading\n",
    "# All other functions can be called after running this script\n",
    "\n",
    "%run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASPIRE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in ASPIRE data dictionary \n",
    " \n",
    "ASPIRE_dict = metadata.load_dictionary_file(path_to_file='../dummy_data/ASPIRE_dictionary_to_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view ASPIRE data dictionary df\n",
    "\n",
    "ASPIRE_dict.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert variables into the database (respond to the prompts with 'y')\n",
    "\n",
    "metadata.insert_variables(engine=engine,formatted_dictionary=ASPIRE_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the metadata that you just inserted into the database\n",
    "# can also do this from the SQL extension \n",
    "\n",
    "pd.read_sql(sql = \"SELECT * FROM cvdnet_consolidated.view_metadata;\",con = engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ASPIRE dummy data (could change to a function for ease of use) - select 200-500 subjects for testing\n",
    "\n",
    "%run ../dummy_data/generate_ASPIRE_dummy_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform raw (dummy) data to template format (this is the only function that needs to be replaced for a new dataset)\n",
    "\n",
    "transform_raw_data.aspire_core_to_template(path_to_file='../dummy_data/ASPIRE_dummy_data.xlsx',output_directory='../dummy_data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in transformed measurement (dummy) data\n",
    "# You may need to update the file path before running as the file you generated will likely have a different date (should be printed just above this box)!\n",
    "\n",
    "ASPIRE_meas = measurements.load_measurement_file(path_to_file=\"../dummy_data/ASPIRE_dummy_data_TRANSFORMED_2025-02_18_160046.csv\")\n",
    "# insert measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPIRE_meas.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the measurements (and subjects) into the database\n",
    "\n",
    "measurements.insert_measurements(formatted_meas=ASPIRE_meas, engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the subjects and measurements that you just inserted into the database\n",
    "\n",
    "pd.read_sql(sql = \"SELECT * FROM cvdnet_consolidated.view_subject_measurements;\",con = engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT-PH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now follow the same workflow for loading the FIT-PH data\n",
    "# Starting with loading in the variables\n",
    "FITPH_dict = metadata.load_dictionary_file(path_to_file=\"../dummy_data/FIT-PH_dictionary_to_template.csv\")\n",
    "metadata.insert_variables(formatted_dictionary=FITPH_dict, engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the new FIT-PH metadata\n",
    "# You can see that the variables are from two sensors (in the category_level_2 column)\n",
    "pd.read_sql(\"SELECT * FROM cvdnet_consolidated.view_metadata WHERE dataset_name = 'FIT-PH';\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate FIT-PH dummy data (could change to a function for ease of use) - select 10 subjects for testing as this dataset is more dense so less subjects used\n",
    "\n",
    "%run ../dummy_data/generate_FIT-PH_dummy_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now transform the dummy data into the template format using a function developed for this FIT-PH sensor data\n",
    "transform_raw_data.fitph_sensor_to_template(path_to_file=\"../dummy_data/FIT-PH_dummy_data.xlsx\", output_directory=\"../dummy_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then load in the transformed file\n",
    "# You may need to update the file path name before running as the file you generated will likely have a different date (should be printed just above this box)!\n",
    "FITPH_meas = measurements.load_measurement_file(path_to_file=\"../dummy_data/FIT-PH_dummy_data_TRANSFORMED_2025-02_18_161214.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert measurements into the database\n",
    "# As it needs to validate a lot of measurements it can be a bit slow (approx 3-6 mins on a laptop - should be faster on TRE, and VMs can be changed if proving to be too slow)\n",
    "measurements.insert_measurements(formatted_meas=FITPH_meas, engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at our new measurements\n",
    "pd.read_sql(sql = \"SELECT * FROM cvdnet_consolidated.view_subject_measurements WHERE dataset_name = 'FIT-PH';\",con = engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for releasing data to the Insights TRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preparing for an export of the data for the Insights TRE\n",
    "# This script will make a second de-identified version of the schema and prepare a database dump of it for export\n",
    "# Script is a shell script so will need to be run from the terminal\n",
    "    # ./prepare_deidentified_database.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check the PostgreSQL Explorer extension and you will see the two schemas (may need to right click and refresh on the database name first)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvdnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
